{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debasish/Debun/agent/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient,models\n",
    "import fitz\n",
    "import re\n",
    "from uuid import uuid4\n",
    "import tiktoken\n",
    "import shutil\n",
    "import numpy as np\n",
    "import bm25s\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def num_tokens_from_string(string, encoding_name = \"cl100k_base\") -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "print(num_tokens_from_string(\"Hello world, let's test tiktoken.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    page_text_lst = [page.get_text(\"text\",sort=True) for page in doc]\n",
    "    return page_text_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple,', ' banana;', ' orange grape.', 'and \\n', 'you for the \\n', 'total']\n"
     ]
    }
   ],
   "source": [
    "# s = \"apple, banana; orange grape\"\n",
    "\n",
    "# # Split using re.finditer to capture delimiters along with words\n",
    "# matches = re.finditer(r'[^;,\\s]+[;,\\s]?', s)\n",
    "\n",
    "# # Combine the words with their respective delimiters\n",
    "# res = [match.group(0) for match in matches]\n",
    "\n",
    "# print(res)\n",
    "\n",
    "\n",
    "s = \"\"\"apple, banana; orange grape.and \n",
    "you for the \n",
    "total\"\"\"\n",
    "\n",
    "def split_docs(string):\n",
    "    delimiters = [',', ';', '\\n\\n','\\n','.']  # List of delimiters \n",
    "\n",
    "    # Create the regex pattern dynamically\n",
    "    pattern = f\"[^{''.join(delimiters)}]+[{'|'.join(delimiters)}]?\"\n",
    "\n",
    "    # Split using re.finditer\n",
    "    matches = re.finditer(pattern, string)\n",
    "\n",
    "    # Combine the words with their respective delimiters\n",
    "    res = [match.group(0) for match in matches]\n",
    "    return res\n",
    "\n",
    "print(split_docs(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_to_chunk(parts, chunk_size=300,min_chunk_size=50):\n",
    "    chunk_1st = []\n",
    "    chunk = \"\"\n",
    "    for i in range(len(parts)): \n",
    "        sub_part = parts[i] \n",
    "        if num_tokens_from_string(sub_part+chunk) < chunk_size: \n",
    "            chunk+=sub_part \n",
    "            if i == len(parts)-1: \n",
    "                chunk_1st.append(chunk)\n",
    "                break\n",
    "        else:\n",
    "            chunk_1st.append(chunk)\n",
    "            chunk = sub_part\n",
    "            if i == len(parts)-1:\n",
    "                chunk_1st.append(chunk)\n",
    "                break\n",
    "    if num_tokens_from_string(chunk_1st[-1]) < min_chunk_size:\n",
    "        last_chunk = chunk_1st.pop()\n",
    "        chunk_1st[-1] = chunk_1st[-1]+last_chunk\n",
    "    return chunk_1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2851, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_page_break_pattern(chunk, pattern):\n",
    "    next_page = False\n",
    "    matches = re.finditer(pattern, chunk)\n",
    "    for match in matches:\n",
    "        value = int(match.group(1))\n",
    "        if match.start() == 0:\n",
    "            next_page = True\n",
    "            return value, next_page\n",
    "\n",
    "        return value, next_page\n",
    "\n",
    "    return -1, False\n",
    "\n",
    "pattern = r\"!@#(\\d+)!@#\"\n",
    "\n",
    "find_page_break_pattern('1@#251!@# total_txt !@#2851!@#ewtrqwr',pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_page_num(list_of_chunk_docs):\n",
    "    prev_page=1\n",
    "    page_details = []\n",
    "    final_chunk_1st = []\n",
    "    page_break_pattern = r\"!@#(\\d+)!@#\" \n",
    "    for i in range(len(list_of_chunk_docs)):\n",
    "        chunk = list_of_chunk_docs[i]#.page_content\n",
    "        chunk_without_page_break = re.sub(page_break_pattern, \"\", chunk)\n",
    "        page_num,next_page = find_page_break_pattern(chunk, page_break_pattern)\n",
    "\n",
    "        if page_num == -1:\n",
    "            # final_chunk = Document(page_content=chunk_without_page_break, metadata {\"file_name\":filename, \"page_details\": prev_page))\n",
    "            page_details.append(prev_page)\n",
    "            final_chunk_1st.append(chunk_without_page_break)\n",
    "        else:\n",
    "            if next_page:\n",
    "                page_details.append(page_num+1)\n",
    "                # final_chunk = Document(page_content=chunk_without_page_break, metadata = {\"file_name\":filename, \"page_details\":page_num+1))\n",
    "\n",
    "                final_chunk_1st.append(chunk_without_page_break)\n",
    "            else:\n",
    "                page_details.append(page_num)\n",
    "                #final_chunk = Document(page_content=chunk_without_page_break, metadata (\"file_name\": filename, \"page_details\":page_num))\n",
    "                final_chunk_1st.append(chunk_without_page_break)\n",
    "\n",
    "            prev_page = page_num+1\n",
    "\n",
    "    return final_chunk_1st,page_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/debasish/Debun/agent/sample_pdfs/gemini 1.5.pdf\"\n",
    "page_chunk_lst = read_pdf(path)\n",
    "total_text = \"\".join(page_chunk_lst[i].strip(\"\\n\")+f\"!@#{i+1}!@#\\n\" for i in range(len(page_chunk_lst)))\n",
    "parts = split_docs(total_text)\n",
    "list_of_chunk_docs = parts_to_chunk(parts)\n",
    "final_chunk_1st,page_details = find_page_num(list_of_chunk_docs)\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "for i,j in zip(final_chunk_1st,page_details):\n",
    "    print(j,i)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([254, 264, 266, 266, 272], [299, 299, 299, 299, 300])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sorted([num_tokens_from_string(i) for i in final_chunk_1st])\n",
    "temp[:5],temp[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(chunks,pages,file_name,ids):\n",
    "    metadata = [] # [{'page_no':i} for i in pages]\n",
    "    documents = [] # [doc for doc in chunks]\n",
    "    corpus_json = []\n",
    "\n",
    "    for doc,page_no,id in zip(chunks,pages,ids):\n",
    "        documents.append(doc)\n",
    "        each_metadata = {'page_no':page_no,\"file_name\":file_name}\n",
    "        metadata.append(each_metadata)\n",
    "        each_dict = {'page_content':doc,\"metadata\":{'page_no':page_no,\"file_name\":file_name,\"id\":id}}\n",
    "        corpus_json.append(each_dict)\n",
    "    return documents, metadata, corpus_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qdrant_dense_emd(documents,metadata,ids,emd_path,collection_name):\n",
    "    if os.path.exists(emd_path): \n",
    "        shutil.rmtree(emd_path)\n",
    "\n",
    "    client = QdrantClient(path = emd_path)\n",
    "    client.set_model(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "    if not client.collection_exists(\"startups\"):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=client.get_fastembed_vector_params()\n",
    "        )\n",
    "    # uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "\n",
    "    # metadata = [{'page_no':i} for i in pages]\n",
    "    # documents = [doc for doc in chunks]\n",
    "\n",
    "    client.add(\n",
    "    collection_name=collection_name,\n",
    "    documents=documents,\n",
    "    metadata=metadata,\n",
    "    ids=ids,\n",
    "    parallel=0,  # Use all available CPU cores to encode data.\n",
    "    # Requires wrapping code into if __name__ == '__main__' block\n",
    "    )\n",
    "    return client\n",
    "    \n",
    "def create_bm25s_db(corpus_json):\n",
    "    corpus_text = [doc['page_content'] for doc in corpus_json]\n",
    "    corpus_tokens = bm25s.tokenize(corpus_text,stopwords='en')\n",
    "    retriever = bm25s.BM25(corpus=corpus_json)\n",
    "    retriever.index(corpus_tokens)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rrf(rank_lists, weights, alpha=60, default_rank=1000, k=5):\n",
    "    all_items = set(item for rank_list in rank_lists for item,_ in rank_list)\n",
    "    item_to_index = {item: idx for idx, item in enumerate(all_items)}\n",
    "    rank_matrix = np.full((len(all_items), len(rank_lists)), default_rank)\n",
    "    for list_idx, rank_list in enumerate(rank_lists):\n",
    "        for item, rank in rank_list:\n",
    "            rank_matrix[item_to_index[item], list_idx] = rank\n",
    "\n",
    "    weighted_rrf_scores = np.sum(weights*(1.0/(alpha + rank_matrix)), axis=1)\n",
    "    sorted_indices = np.argsort(-weighted_rrf_scores) # Negative for descending order\n",
    "    sorted_items = [(list(item_to_index.keys()) [idx], weighted_rrf_scores [idx]) for idx in sorted_indices]\n",
    "\n",
    "    return sorted_items[:k]\n",
    "\n",
    "\n",
    "def get_doc_and_source(rrf_retriever, retrieve_doc_dict_keyword, retrieve_doc_dict_sim_search):\n",
    "    final_retrieve_lst = []\n",
    "    unique_source = []\n",
    "    all_source= []\n",
    "\n",
    "    for final_retrieve_doc_with_score in rrf_retriever:\n",
    "        final_retrieve_doc = final_retrieve_doc_with_score[0]\n",
    "        final_retrieve_lst.append(final_retrieve_doc)\n",
    "\n",
    "        if final_retrieve_doc in list(retrieve_doc_dict_keyword.keys()): \n",
    "            source = retrieve_doc_dict_keyword[final_retrieve_doc]\n",
    "            all_source.append(source)\n",
    "            if source not in unique_source:\n",
    "                unique_source.append(source)\n",
    "\n",
    "        elif final_retrieve_doc in list(retrieve_doc_dict_sim_search.keys()):\n",
    "            source = retrieve_doc_dict_sim_search[final_retrieve_doc] \n",
    "            all_source.append(source) \n",
    "            if source not in unique_source:\n",
    "                unique_source.append(source) \n",
    "    return final_retrieve_lst, unique_source, all_source\n",
    "\n",
    "\n",
    "def prepare_retrieve_doc(final_retrieve_lst,all_source):\n",
    "    i = 1\n",
    "    context = \"\"\n",
    "    for doc,source in zip(final_retrieve_lst,all_source):\n",
    "        context += doc.strip(\"\\n\")\n",
    "        context += \"\\n-----------------\\n\"\n",
    "        i+=1\n",
    "    return context.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    }
   ],
   "source": [
    "keyword_retriever = create_bm25s_db(corpus_json)\n",
    "retrieve_doc,retrieve_doc_dict = keyword_search(query,keyword_retriever)\n",
    "\n",
    "# retrieve_doc,retrieve_doc_dict = similarity_search(query,client,collection_name,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query,client,collection_name,k=5):\n",
    "    retrieve_doc = []\n",
    "    retrieve_doc_dict = {}\n",
    "    retrieve_docs = client.query(collection_name = collection_name,query_text = query,limit = k)\n",
    "    rank = 1\n",
    "    for doc in retrieve_docs:\n",
    "        id = doc.id\n",
    "        page_content = doc.metadata['document']\n",
    "        metadata = {'id':id,'page_no':doc.metadata['page_no'],'file_name':doc.metadata['file_name']}\n",
    "        retrieve_doc.append((page_content,rank))\n",
    "        file_name = metadata['file_name']\n",
    "        page_no = metadata['page_no']\n",
    "        path = os.path.join(\"uploaded_files\",file_name)\n",
    "        source = f\"{path}#page={page_no}\"\n",
    "        retrieve_doc_dict[page_content] = source\n",
    "        rank+=1\n",
    "    return retrieve_doc,retrieve_doc_dict\n",
    "\n",
    "def keyword_search(query,keyword_retriever,k=5):\n",
    "    # keyword_retriever = bm25s.BM25.load(path,load_corpus=True)\n",
    "    query_tokens = bm25s.tokenize(query)\n",
    "    results,scores = keyword_retriever.retrieve(query_tokens,k=k)\n",
    "    retrieve_doc = []\n",
    "    retrieve_doc_dict = {}\n",
    "    rank = 1\n",
    "    for doc in results[0]:\n",
    "        page_content = doc['page_content']\n",
    "        metadata = doc['metadata']\n",
    "        retrieve_doc.append((page_content,rank))\n",
    "        file_name = metadata['file_name']\n",
    "        page_no = metadata['page_no']\n",
    "        path = os.path.join(\"uploaded_files\",file_name)\n",
    "        source = f\"{path}#page={page_no}\"\n",
    "        retrieve_doc_dict[page_content] = source\n",
    "        rank+=1\n",
    "    return retrieve_doc,retrieve_doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ensemble_retriever (query,k, weights,client,collection_name,keyword_retriever):\n",
    "    retrieve_doc_sim_search, retrieve_doc_dict_sim_search = similarity_search(query,client,collection_name,k=5)\n",
    "    retrieve_doc_keyword, retrieve_doc_dict_keyword = keyword_search(query,keyword_retriever)\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    rrf_retriever = weighted_rrf([retrieve_doc_keyword, retrieve_doc_sim_search], weights, k=k)\n",
    "    final_retrieve_lst, unique_source, all_source = get_doc_and_source(rrf_retriever, retrieve_doc_dict_keyword, retrieve_doc_dict_sim_search) \n",
    "    retrieve_context = prepare_retrieve_doc(final_retrieve_lst, all_source)\n",
    "\n",
    "    return retrieve_context, unique_source,all_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Large language models have been showed to reproduce and amplify biases that are existing in\n",
    "the training data (Sheng et al., 2019; Kurita et al.,\n",
    "2019), and to generate toxic or offensive content (Gehman et al., 2020). \n",
    "\"\"\"\n",
    "retrieve_context, unique_source,all_source = custom_ensemble_retriever(query=query,k=5,weights=weights,\n",
    "                                                            client=client,collection_name=collection_name,\n",
    "                                                            keyword_retriever=keyword_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uploaded_files/temp.pdf#page=7',\n",
       " 'uploaded_files/temp.pdf#page=8',\n",
       " 'uploaded_files/temp.pdf#page=7',\n",
       " 'uploaded_files/temp.pdf#page=11',\n",
       " 'uploaded_files/temp.pdf#page=6']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieve_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = client.query(collection_name = collection_name,query_text = query,limit = 5)\n",
    "metadata = [(hit.id,hit.metadata) for hit in doc]\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output. We use the RMSNorm normalizing func-\\ntion, introduced by Zhang and Sennrich (2019).      2.4  Efﬁcient implementation\\nSwiGLU activation function [PaLM]. We re-  We make several optimizations to improve the train-\\nplace the ReLU non-linearity by the SwiGLU ac-   ing speed of our models. First, we use an efﬁcient\\ntivation function, introduced by Shazeer (2020) to   implementation of the causal multi-head attention\\nimprove the performance. We use a dimension of    to reduce memory usage and runtime. This imple-\\n234d instead of 4d as in PaLM.                      mentation, available in the xformers library,2 is\\n                                                     inspired by Rabe and Staats (2021) and uses the\\nRotary Embeddings [GPTNeo]. We remove the                                             backward from Dao et al. (2022). This is achieved\\nabsolute positional embeddings, and instead, add                                          by not storing the attention weights and not com-\\nrotary positional embeddings (RoPE), introduced                                                   puting the key/query scores that are masked due to\\nby Su et al. (2021), at each layer of the network.                                                     the causal nature of the language modeling task.  The details of the hyper-parameters for our dif-                                           To further improve training efﬁciency, we re-\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(doc[0].metadata['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(id='5c3cd82d54b947488694965eaff735dc', embedding=None, sparse_embedding=None, metadata={'document': 'output. We use the RMSNorm normalizing func-\\ntion, introduced by Zhang and Sennrich (2019).      2.4  Efﬁcient implementation\\nSwiGLU activation function [PaLM]. We re-  We make several optimizations to improve the train-\\nplace the ReLU non-linearity by the SwiGLU ac-   ing speed of our models. First, we use an efﬁcient\\ntivation function, introduced by Shazeer (2020) to   implementation of the causal multi-head attention\\nimprove the performance. We use a dimension of    to reduce memory usage and runtime. This imple-\\n234d instead of 4d as in PaLM.                      mentation, available in the xformers library,2 is\\n                                                     inspired by Rabe and Staats (2021) and uses the\\nRotary Embeddings [GPTNeo]. We remove the                                             backward from Dao et al. (2022). This is achieved\\nabsolute positional embeddings, and instead, add                                          by not storing the attention weights and not com-\\nrotary positional embeddings (RoPE), introduced                                                   puting the key/query scores that are masked due to\\nby Su et al. (2021), at each layer of the network.                                                     the causal nature of the language modeling task.  The details of the hyper-parameters for our dif-                                           To further improve training efﬁciency, we re-\\n', 'page_no': 3}, document='output. We use the RMSNorm normalizing func-\\ntion, introduced by Zhang and Sennrich (2019).      2.4  Efﬁcient implementation\\nSwiGLU activation function [PaLM]. We re-  We make several optimizations to improve the train-\\nplace the ReLU non-linearity by the SwiGLU ac-   ing speed of our models. First, we use an efﬁcient\\ntivation function, introduced by Shazeer (2020) to   implementation of the causal multi-head attention\\nimprove the performance. We use a dimension of    to reduce memory usage and runtime. This imple-\\n234d instead of 4d as in PaLM.                      mentation, available in the xformers library,2 is\\n                                                     inspired by Rabe and Staats (2021) and uses the\\nRotary Embeddings [GPTNeo]. We remove the                                             backward from Dao et al. (2022). This is achieved\\nabsolute positional embeddings, and instead, add                                          by not storing the attention weights and not com-\\nrotary positional embeddings (RoPE), introduced                                                   puting the key/query scores that are masked due to\\nby Su et al. (2021), at each layer of the network.                                                     the causal nature of the language modeling task.  The details of the hyper-parameters for our dif-                                           To further improve training efﬁciency, we re-\\n', score=0.7772807708572015)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QdrantClient.search() missing 2 required positional arguments: 'collection_name' and 'query_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m doc\n",
      "\u001b[0;31mTypeError\u001b[0m: QdrantClient.search() missing 2 required positional arguments: 'collection_name' and 'query_vector'"
     ]
    }
   ],
   "source": [
    "doc = client.search(collection_name=\"startups\",query)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearcher:\n",
    "    DENSE_MODEL = \"BAAI/bge-base-en-v1.5\"\n",
    "    # SPARSE_MODEL = \"prithivida/Splade_PP_en_v1\"\n",
    "    def __init__(self, collection_name):\n",
    "        self.collection_name = collection_name\n",
    "        # initialize Qdrant client\n",
    "        self.qdrant_client = QdrantClient(path=\"dense\")\n",
    "        self.qdrant_client.set_model(self.DENSE_MODEL)\n",
    "        # comment this line to use dense vectors only\n",
    "        # self.qdrant_client.set_sparse_model(self.SPARSE_MODEL)\n",
    "    def search(self, text: str):\n",
    "      search_result = self.qdrant_client.query(\n",
    "          collection_name=self.collection_name,\n",
    "          query_text=text,\n",
    "          query_filter=None,  # If you don't want any filters for now\n",
    "          limit=5,  # 5 the closest results\n",
    "      )\n",
    "      # `search_result` contains found vector ids with similarity scores\n",
    "      # along with the stored payload\n",
    "\n",
    "      # Select and return metadata\n",
    "      metadata = [hit.metadata for hit in search_result]\n",
    "      return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'We make several optimizations to improve the training speed of our models. First, we use an efficient implementation of the causal multi-head attention'\n",
    "hybrid_searcher = SemanticSearcher(collection_name=\"startups\")\n",
    "doc = hybrid_searcher.search(text=query)\n",
    "doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
