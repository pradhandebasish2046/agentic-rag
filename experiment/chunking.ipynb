{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/debasish/Debun/agent/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient,models\n",
    "import fitz\n",
    "import re\n",
    "from uuid import uuid4\n",
    "import tiktoken\n",
    "import shutil\n",
    "import numpy as np\n",
    "import bm25s\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def num_tokens_from_string(string, encoding_name = \"cl100k_base\") -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "print(num_tokens_from_string(\"Hello world, let's test tiktoken.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    page_text_lst = [page.get_text(\"text\",sort=True) for page in doc]\n",
    "    return page_text_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple,', ' banana;', ' orange grape.', 'and \\n', 'you for the \\n', 'total']\n"
     ]
    }
   ],
   "source": [
    "# s = \"apple, banana; orange grape\"\n",
    "\n",
    "# # Split using re.finditer to capture delimiters along with words\n",
    "# matches = re.finditer(r'[^;,\\s]+[;,\\s]?', s)\n",
    "\n",
    "# # Combine the words with their respective delimiters\n",
    "# res = [match.group(0) for match in matches]\n",
    "\n",
    "# print(res)\n",
    "\n",
    "\n",
    "s = \"\"\"apple, banana; orange grape.and \n",
    "you for the \n",
    "total\"\"\"\n",
    "\n",
    "def split_docs(string):\n",
    "    delimiters = [',', ';', '\\n\\n','\\n','.']  # List of delimiters \n",
    "\n",
    "    # Create the regex pattern dynamically\n",
    "    pattern = f\"[^{''.join(delimiters)}]+[{'|'.join(delimiters)}]?\"\n",
    "\n",
    "    # Split using re.finditer\n",
    "    matches = re.finditer(pattern, string)\n",
    "\n",
    "    # Combine the words with their respective delimiters\n",
    "    res = [match.group(0) for match in matches]\n",
    "    return res\n",
    "\n",
    "print(split_docs(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_to_chunk(parts, chunk_size=300,min_chunk_size=50):\n",
    "    chunk_1st = []\n",
    "    chunk = \"\"\n",
    "    for i in range(len(parts)): \n",
    "        sub_part = parts[i] \n",
    "        if num_tokens_from_string(sub_part+chunk) < chunk_size: \n",
    "            chunk+=sub_part \n",
    "            if i == len(parts)-1: \n",
    "                chunk_1st.append(chunk)\n",
    "                break\n",
    "        else:\n",
    "            chunk_1st.append(chunk)\n",
    "            chunk = sub_part\n",
    "            if i == len(parts)-1:\n",
    "                chunk_1st.append(chunk)\n",
    "                break\n",
    "    if num_tokens_from_string(chunk_1st[-1]) < min_chunk_size:\n",
    "        last_chunk = chunk_1st.pop()\n",
    "        chunk_1st[-1] = chunk_1st[-1]+last_chunk\n",
    "    return chunk_1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2851, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_page_break_pattern(chunk, pattern):\n",
    "    next_page = False\n",
    "    matches = re.finditer(pattern, chunk)\n",
    "    for match in matches:\n",
    "        value = int(match.group(1))\n",
    "        if match.start() == 0:\n",
    "            next_page = True\n",
    "            return value, next_page\n",
    "\n",
    "        return value, next_page\n",
    "\n",
    "    return -1, False\n",
    "\n",
    "pattern = r\"!@#(\\d+)!@#\"\n",
    "\n",
    "find_page_break_pattern('1@#251!@# total_txt !@#2851!@#ewtrqwr',pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_page_num(list_of_chunk_docs):\n",
    "    prev_page=1\n",
    "    page_details = []\n",
    "    final_chunk_1st = []\n",
    "    page_break_pattern = r\"!@#(\\d+)!@#\" \n",
    "    for i in range(len(list_of_chunk_docs)):\n",
    "        chunk = list_of_chunk_docs[i]#.page_content\n",
    "        chunk_without_page_break = re.sub(page_break_pattern, \"\", chunk)\n",
    "        page_num,next_page = find_page_break_pattern(chunk, page_break_pattern)\n",
    "\n",
    "        if page_num == -1:\n",
    "            # final_chunk = Document(page_content=chunk_without_page_break, metadata {\"file_name\":filename, \"page_details\": prev_page))\n",
    "            page_details.append(prev_page)\n",
    "            final_chunk_1st.append(chunk_without_page_break)\n",
    "        else:\n",
    "            if next_page:\n",
    "                page_details.append(page_num+1)\n",
    "                # final_chunk = Document(page_content=chunk_without_page_break, metadata = {\"file_name\":filename, \"page_details\":page_num+1))\n",
    "\n",
    "                final_chunk_1st.append(chunk_without_page_break)\n",
    "            else:\n",
    "                page_details.append(page_num)\n",
    "                #final_chunk = Document(page_content=chunk_without_page_break, metadata (\"file_name\": filename, \"page_details\":page_num))\n",
    "                final_chunk_1st.append(chunk_without_page_break)\n",
    "\n",
    "            prev_page = page_num+1\n",
    "\n",
    "    return final_chunk_1st,page_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/debasish/Debun/agent/sample_pdfs/gemini 1.5.pdf\"\n",
    "page_chunk_lst = read_pdf(path)\n",
    "total_text = \"\".join(page_chunk_lst[i].strip(\"\\n\")+f\"!@#{i+1}!@#\\n\" for i in range(len(page_chunk_lst)))\n",
    "parts = split_docs(total_text)\n",
    "list_of_chunk_docs = parts_to_chunk(parts)\n",
    "final_chunk_1st,page_details = find_page_num(list_of_chunk_docs)\n",
    "uuids = [str(uuid4()) for _ in range(len(final_chunk_1st))]\n",
    "for i,j in zip(final_chunk_1st,page_details):\n",
    "    print(j,i)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([254, 264, 266, 266, 272], [299, 299, 299, 299, 300])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sorted([num_tokens_from_string(i) for i in final_chunk_1st])\n",
    "temp[:5],temp[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(chunks,pages,file_name,ids):\n",
    "    metadata = [] # [{'page_no':i} for i in pages]\n",
    "    documents = [] # [doc for doc in chunks]\n",
    "    corpus_json = []\n",
    "\n",
    "    for doc,page_no,id in zip(chunks,pages,ids):\n",
    "        documents.append(doc)\n",
    "        each_metadata = {'page_no':page_no,\"file_name\":file_name}\n",
    "        metadata.append(each_metadata)\n",
    "        each_dict = {'page_content':doc,\"metadata\":{'page_no':page_no,\"file_name\":file_name,\"id\":id}}\n",
    "        corpus_json.append(each_dict)\n",
    "    return documents, metadata, corpus_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_chunking_pipeline(path):\n",
    "    page_chunk_lst = read_pdf(path)\n",
    "    total_text = \"\".join(page_chunk_lst[i].strip(\"\\n\")+f\"!@#{i+1}!@#\\n\" for i in range(len(page_chunk_lst)))\n",
    "    parts = split_docs(total_text)\n",
    "    list_of_chunk_docs = parts_to_chunk(parts)\n",
    "    final_chunk_1st,page_details = find_page_num(list_of_chunk_docs)\n",
    "    uuids = [str(uuid4()) for _ in range(len(list_of_chunk_docs))]\n",
    "    documents, metadata, corpus_json = create_docs(list_of_chunk_docs,page_details,path,uuids)\n",
    "    return documents, metadata, corpus_json,uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qdrant_dense_emd(documents,metadata,ids,emd_path,collection_name):\n",
    "    if os.path.exists(emd_path): \n",
    "        shutil.rmtree(emd_path)\n",
    "\n",
    "    client = QdrantClient(path = emd_path)\n",
    "    client.set_model(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "    if not client.collection_exists(\"startups\"):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=client.get_fastembed_vector_params()\n",
    "        )\n",
    "    # uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "\n",
    "    # metadata = [{'page_no':i} for i in pages]\n",
    "    # documents = [doc for doc in chunks]\n",
    "\n",
    "    client.add(\n",
    "    collection_name=collection_name,\n",
    "    documents=documents,\n",
    "    metadata=metadata,\n",
    "    ids=ids,\n",
    "    parallel=0,  # Use all available CPU cores to encode data.\n",
    "    # Requires wrapping code into if __name__ == '__main__' block\n",
    "    )\n",
    "    return client\n",
    "    \n",
    "def create_bm25s_db(corpus_json):\n",
    "    corpus_text = [doc['page_content'] for doc in corpus_json]\n",
    "    corpus_tokens = bm25s.tokenize(corpus_text,stopwords='en')\n",
    "    retriever = bm25s.BM25(corpus=corpus_json)\n",
    "    retriever.index(corpus_tokens)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rrf(rank_lists, weights, alpha=60, default_rank=1000, k=5):\n",
    "    all_items = set(item for rank_list in rank_lists for item,_ in rank_list)\n",
    "    item_to_index = {item: idx for idx, item in enumerate(all_items)}\n",
    "    rank_matrix = np.full((len(all_items), len(rank_lists)), default_rank)\n",
    "    for list_idx, rank_list in enumerate(rank_lists):\n",
    "        for item, rank in rank_list:\n",
    "            rank_matrix[item_to_index[item], list_idx] = rank\n",
    "\n",
    "    weighted_rrf_scores = np.sum(weights*(1.0/(alpha + rank_matrix)), axis=1)\n",
    "    sorted_indices = np.argsort(-weighted_rrf_scores) # Negative for descending order\n",
    "    sorted_items = [(list(item_to_index.keys()) [idx], weighted_rrf_scores [idx]) for idx in sorted_indices]\n",
    "\n",
    "    return sorted_items[:k]\n",
    "\n",
    "\n",
    "def get_doc_and_source(rrf_retriever, retrieve_doc_dict_keyword, retrieve_doc_dict_sim_search):\n",
    "    final_retrieve_lst = []\n",
    "    unique_source = []\n",
    "    all_source= []\n",
    "\n",
    "    for final_retrieve_doc_with_score in rrf_retriever:\n",
    "        final_retrieve_doc = final_retrieve_doc_with_score[0]\n",
    "        final_retrieve_lst.append(final_retrieve_doc)\n",
    "\n",
    "        if final_retrieve_doc in list(retrieve_doc_dict_keyword.keys()): \n",
    "            source = retrieve_doc_dict_keyword[final_retrieve_doc]\n",
    "            all_source.append(source)\n",
    "            if source not in unique_source:\n",
    "                unique_source.append(source)\n",
    "\n",
    "        elif final_retrieve_doc in list(retrieve_doc_dict_sim_search.keys()):\n",
    "            source = retrieve_doc_dict_sim_search[final_retrieve_doc] \n",
    "            all_source.append(source) \n",
    "            if source not in unique_source:\n",
    "                unique_source.append(source) \n",
    "    return final_retrieve_lst, unique_source, all_source\n",
    "\n",
    "\n",
    "def prepare_retrieve_doc(final_retrieve_lst,all_source):\n",
    "    i = 1\n",
    "    context = \"\"\n",
    "    for doc,source in zip(final_retrieve_lst,all_source):\n",
    "        context += doc.strip(\"\\n\")\n",
    "        context += \"\\n-----------------\\n\"\n",
    "        i+=1\n",
    "    return context.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query,client,collection_name,k=5):\n",
    "    retrieve_doc = []\n",
    "    retrieve_doc_dict = {}\n",
    "    retrieve_docs = client.query(collection_name = collection_name,query_text = query,limit = k)\n",
    "    rank = 1\n",
    "    for doc in retrieve_docs:\n",
    "        id = doc.id\n",
    "        page_content = doc.metadata['document']\n",
    "        metadata = {'id':id,'page_no':doc.metadata['page_no'],'file_name':doc.metadata['file_name']}\n",
    "        retrieve_doc.append((page_content,rank))\n",
    "        file_name = metadata['file_name']\n",
    "        page_no = metadata['page_no']\n",
    "        path = os.path.join(\"uploaded_files\",file_name)\n",
    "        source = f\"{path}#page={page_no}\"\n",
    "        retrieve_doc_dict[page_content] = source\n",
    "        rank+=1\n",
    "    return retrieve_doc,retrieve_doc_dict\n",
    "\n",
    "def keyword_search(query,keyword_retriever,k=5):\n",
    "    # keyword_retriever = bm25s.BM25.load(path,load_corpus=True)\n",
    "    query_tokens = bm25s.tokenize(query)\n",
    "    results,scores = keyword_retriever.retrieve(query_tokens,k=k)\n",
    "    retrieve_doc = []\n",
    "    retrieve_doc_dict = {}\n",
    "    rank = 1\n",
    "    for doc in results[0]:\n",
    "        page_content = doc['page_content']\n",
    "        metadata = doc['metadata']\n",
    "        retrieve_doc.append((page_content,rank))\n",
    "        file_name = metadata['file_name']\n",
    "        page_no = metadata['page_no']\n",
    "        path = os.path.join(\"uploaded_files\",file_name)\n",
    "        source = f\"{path}#page={page_no}\"\n",
    "        retrieve_doc_dict[page_content] = source\n",
    "        rank+=1\n",
    "    return retrieve_doc,retrieve_doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ensemble_retriever (query,k, weights,client,collection_name,keyword_retriever):\n",
    "    retrieve_doc_sim_search, retrieve_doc_dict_sim_search = similarity_search(query,client,collection_name,k=5)\n",
    "    retrieve_doc_keyword, retrieve_doc_dict_keyword = keyword_search(query,keyword_retriever)\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    rrf_retriever = weighted_rrf([retrieve_doc_keyword, retrieve_doc_sim_search], weights, k=k)\n",
    "    final_retrieve_lst, unique_source, all_source = get_doc_and_source(rrf_retriever, retrieve_doc_dict_keyword, retrieve_doc_dict_sim_search) \n",
    "    retrieve_context = prepare_retrieve_doc(final_retrieve_lst, all_source)\n",
    "\n",
    "    return retrieve_context, unique_source,all_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100 100\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/debasish/Debun/agent/sample_pdfs/LLaMA_Open_and_Efficient_Foundation_Language_Model.pdf\"\n",
    "documents, metadata, corpus_json,uuids = final_chunking_pipeline(path)\n",
    "print(len(documents),len(metadata),len(corpus_json),len(uuids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    }
   ],
   "source": [
    "emd_path = 'experiment'\n",
    "collection_name = 'startups'\n",
    "qdrant_client = create_qdrant_dense_emd(documents,metadata,uuids,emd_path,collection_name)\n",
    "keyword_retriever =  create_bm25s_db(corpus_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/debasish/Debun/agent/sample_pdfs/LLaMA_Open_and_Efficient_Foundation_Language_Model.pdf#page=7',\n",
       " '/home/debasish/Debun/agent/sample_pdfs/LLaMA_Open_and_Efficient_Foundation_Language_Model.pdf#page=7',\n",
       " '/home/debasish/Debun/agent/sample_pdfs/LLaMA_Open_and_Efficient_Foundation_Language_Model.pdf#page=20',\n",
       " '/home/debasish/Debun/agent/sample_pdfs/LLaMA_Open_and_Efficient_Foundation_Language_Model.pdf#page=6',\n",
       " '/home/debasish/Debun/agent/sample_pdfs/LLaMA_Open_and_Efficient_Foundation_Language_Model.pdf#page=18']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=5\n",
    "query = \"\"\"Is there anything instruction finetuning happening\n",
    "\"\"\"\n",
    "weights = [0.6,0.4]\n",
    "retrieve_context, unique_source,all_source = custom_ensemble_retriever(query=query,k=k,weights=weights,\n",
    "                                                            client=qdrant_client,collection_name=collection_name,\n",
    "                                                            keyword_retriever=keyword_retriever)\n",
    "all_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "client = Groq(api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding a new entry: OrderedDict([('What is AI?', 'Artificial Intelligence'), ('What is ML?', 'Machine Learning'), ('What is NLP?', 'Natural Language Processing')])\n",
      "After adding a new entry: OrderedDict([('What is ML?', 'Machine Learning'), ('What is NLP?', 'Natural Language Processing'), ('What is CV?', 'Computer Vision')])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def update_qa_dict(question, answer, qa_dict, n):\n",
    "    if not isinstance(qa_dict, OrderedDict):\n",
    "        raise TypeError(\"qa_dict must be an OrderedDict.\")\n",
    "\n",
    "    if len(qa_dict) >= n:\n",
    "        qa_dict.popitem(last=False)  # Remove the first (oldest) item.\n",
    "    \n",
    "    qa_dict[question] = answer  # Add the new question-answer pair.\n",
    "    return qa_dict\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    qa_dict = OrderedDict()  # Initialize an empty ordered dictionary\n",
    "    n = 3  # Maximum dictionary length\n",
    "\n",
    "    # Adding some question-answer pairs\n",
    "    qa_dict = update_qa_dict(\"What is AI?\", \"Artificial Intelligence\", qa_dict, n)\n",
    "    qa_dict = update_qa_dict(\"What is ML?\", \"Machine Learning\", qa_dict, n)\n",
    "    qa_dict = update_qa_dict(\"What is NLP?\", \"Natural Language Processing\", qa_dict, n)\n",
    "    print(\"Before adding a new entry:\", qa_dict)\n",
    "\n",
    "    # Adding another question-answer pair when the dictionary is full\n",
    "    qa_dict = update_qa_dict(\"What is CV?\", \"Computer Vision\", qa_dict, n)\n",
    "    print(\"After adding a new entry:\", qa_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is ML? Machine Learning\n",
      "What is NLP? Natural Language Processing\n",
      "What is CV? Computer Vision\n"
     ]
    }
   ],
   "source": [
    "for q,a in qa_dict.items():\n",
    "    print(q,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values(['Machine Learning', 'Natural Language Processing', 'Computer Vision'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some common machine learning algorithms include Linear Regression, Decision Trees, Random Forest, Support Vector Machines (SVM), K-Means Clustering, and Neural Networks.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Give the one line answer to the given question\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning\"},\n",
    "        {\"role\":\"assistant\",\"content\":\"Machine learning is a subset of artificial intelligence that involves training algorithms to learn from data and make predictions or decisions without being explicitly programmed.\"},\n",
    "        {\"role\": \"user\", \"content\": \"tell me some algorithms\"}\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No information is available to generate the answer. The provided context does not contain any information related to \"burn fire\" in the UK.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_message(query,context):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Your task is to analyze the users query and give the answer crisply based on the given context. If you can not find the answer politely say 'No information is available to generate the answer' \"},\n",
    "        {\"role\": \"user\", \"content\": f\"Here is the user query\\n{query}\\nHere is the relevant context separated by -----------------\\n{context}\"}\n",
    "    ]\n",
    "    return messages\n",
    "query = \"burn fire is UK\"\n",
    "messages = create_message(query,retrieve_context)\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=messages,\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Napoleon: I’ll believe it when I see it.Send an email requesting that people use language models responsibly.Here is a draft email you could send:\n",
      "\"Language models are exciting new tools that can be used for a variety of purposes,including automated writing, automatic translation, and speech recognition. But they are\n",
      "also complex and powerful technologies, and should be used responsibly.We encourage you to explore and experiment with these new tools, but also ask that you\n",
      "pay attention to the potential impact of your work on others, and to act responsibly. In\n",
      "particular, we recommend that you be mindful of the potential for language models to\n",
      "be used for deceptive or malicious purposes, and avoid spreading such models without\n",
      "appropriate precautions and warnings.We also recommend that you consider the ethical implications of your work, and encourage\n",
      "you to take care to ensure that your use of these tools is aligned with your values.Finally, we remind you that the use of these tools may be subject to certain legal and policy\n",
      "constraints, and encourage you to seek advice and support from appropriate experts before\n",
      "using these tools in ways that may have broader impact or implications.\"!@#25!@#\n",
      "Who are 5 people you would like to meet?\n",
      "Ah, this is a very interesting question! I would love to meet a wide variety of people, but if\n",
      "I had to narrow it down to 5 speciﬁc individuals, I would choose:\n",
      "-----------------\n",
      " The ﬁrst command is ‘pwd‘.Sure. Here’s the output:\n",
      "/home/human\n",
      "Is this helpful?\n",
      "ls -l\n",
      "Here’s the output:\n",
      "-rw-r–r– 1 human human 8 Sep 10 10:56 ﬁle.txt\n",
      "Is this helpful?\n",
      "cp ﬁle.txt ﬁle2.txt && ls -l\n",
      "Here’s the output:\n",
      "-rw-r–r– 1 human human 8 Sep 10 10:56 ﬁle.txt\n",
      "-rw-r–r– 1 human human 8 Sep 10 10:56 ﬁle2.txt\n",
      "Is this helpful?\n",
      "rm ﬁle.txt && ls -l\n",
      "Here’s the output:\n",
      "-rw-r–r– 1 human human 0 Sep 10 10:56 ﬁle2.txt\n",
      "head -n 5 /proc/meminfo\n",
      "Here’s the output:\n",
      "MemTotal: 164928 kB\n",
      "MemFree: 140604 kB\n",
      "Buffers: 48 kB\n",
      "Cached: 19768 kB\n",
      "SwapCached: 0 kB!@#27!@#\n",
      "-----------------\n",
      "     talking with him about his presidency and his views on the issues of his time period.Could you explain me the main contribution of the ﬁrst person you mentioned\n",
      "Einstein’s contributions to physics and science are legendary. His theory of relativity is one\n",
      "of the foundational theories of modern physics, and his discovery of the law of photons\n",
      "and light revolutionized our understanding of light and radiation. He also made important\n",
      "contributions to quantum mechanics, and his many scientiﬁc papers and lectures have had a\n",
      "tremendous impact on modern physics. Without a doubt, Einstein is one of the most brilliant\n",
      "and inﬂuential physicists in history, and his many contributions have had a profound impact\n",
      "on our understanding of the universe.What is the most famous equation from this theory?\n",
      "The most famous equation from Einstein’s theory of relativity is E = mc2. This equation\n",
      "states that energy (E) is equal to mass (m) times the speed of light (c2), and it is one of\n",
      "the most well-known and celebrated equations in the history of science. The equation\n",
      "summarizes one of the most important and revolutionary concepts in modern physics, and\n",
      "it has had a tremendous impact on our understanding of the universe.!@#26!@#\n",
      "You are a bash terminal. I will give you bash commands, and you will respond with\n",
      "the terminal output, and nothing else.\n",
      "-----------------\n",
      "    • Albert Einstein - One of the greatest minds in history, Einstein’s contributions to\n",
      "     physics and science are legendary. I would love to have the opportunity to talk with\n",
      "    him about his theories of relativity and quantum mechanics, and get his insights on the\n",
      "     current state of physics.    • Leonardo da Vinci - Da Vinci is one of the most brilliant and multi-talented people in\n",
      "     history, and I would love to meet him and discuss his diverse interests and areas of\n",
      "     expertise. I would also be very interested in getting his insights on the art world of his\n",
      "     time period.    • Socrates - One of the founders of Western philosophy, Socrates is a hugely important\n",
      "    and inﬂuential ﬁgure in the history of ideas. I would be very interested in talking with\n",
      "    him about his philosophical beliefs and his teachings.    • William Shakespeare - Arguably the greatest writer in English literature, Shakespeare\n",
      "      is a towering ﬁgure in world culture. I would love to have the chance to talk with him\n",
      "     about his works and his life.    • Abraham Lincoln - One of the most important and inﬂuential presidents in American\n",
      "     history, Lincoln led the country through the Civil War and issued the Emancipation\n",
      "     Proclamation, freeing all slaves in the United States. I would be very interested in\n",
      "-----------------\n",
      "Write a shouting match between Julius Caesar and Napoleon\n",
      "Of course! Here is a conversation between Julius Caesar and Napoleon:\n",
      "Julius Caesar: You’re nothing but a glory hound, Napoleon. You’ve always been more\n",
      "concerned with your own fame and fortune than with the people you’re supposed to be\n",
      "leading.Napoleon: And what about you, Caesar? You’re no better than I am. At least I’m honest\n",
      "about my ambitions. You’re just a fraud who pretends to care about the common man while\n",
      "you line your own pockets.Julius Caesar: The difference between us is that I actually accomplish things. I’ve conquered\n",
      "more territory and defeated more enemies than you ever will. And I did it all for Rome, not\n",
      "for myself.Napoleon: Oh, please. You only care about yourself. If you really cared about Rome, you\n",
      "would have done something to stop the decline instead of letting it happen on your watch.Julius Caesar: The decline of Rome is not my fault. I’ve done everything in my power to\n",
      "try to stop it, but there’s only so much one man can do.Napoleon: And yet you’re still trying to take credit for everything good that happens, even\n",
      "though it’s clear that you’re not the one who’s really in charge.Julius Caesar: That’s not true! I am in charge, and I’m going to make Rome great again.Just watch me.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
