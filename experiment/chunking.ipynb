{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient,models\n",
    "import fitz\n",
    "import re\n",
    "from uuid import uuid4\n",
    "import tiktoken\n",
    "import shutil\n",
    "import numpy as np\n",
    "import bm25s\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def num_tokens_from_string(string, encoding_name = \"cl100k_base\") -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "print(num_tokens_from_string(\"Hello world, let's test tiktoken.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    page_text_lst = [page.get_text(\"text\",sort=True) for page in doc]\n",
    "    return page_text_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple,', ' banana;', ' orange grape.', 'and \\n', 'you for the \\n', 'total']\n"
     ]
    }
   ],
   "source": [
    "# s = \"apple, banana; orange grape\"\n",
    "\n",
    "# # Split using re.finditer to capture delimiters along with words\n",
    "# matches = re.finditer(r'[^;,\\s]+[;,\\s]?', s)\n",
    "\n",
    "# # Combine the words with their respective delimiters\n",
    "# res = [match.group(0) for match in matches]\n",
    "\n",
    "# print(res)\n",
    "\n",
    "\n",
    "s = \"\"\"apple, banana; orange grape.and \n",
    "you for the \n",
    "total\"\"\"\n",
    "\n",
    "def split_docs(string):\n",
    "    delimiters = [',', ';', '\\n\\n','\\n','.']  # List of delimiters \n",
    "\n",
    "    # Create the regex pattern dynamically\n",
    "    pattern = f\"[^{''.join(delimiters)}]+[{'|'.join(delimiters)}]?\"\n",
    "\n",
    "    # Split using re.finditer\n",
    "    matches = re.finditer(pattern, string)\n",
    "\n",
    "    # Combine the words with their respective delimiters\n",
    "    res = [match.group(0) for match in matches]\n",
    "    return res\n",
    "\n",
    "print(split_docs(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_to_chunk(parts, chunk_size=300,min_chunk_size=50):\n",
    "    chunk_1st = []\n",
    "    chunk = \"\"\n",
    "    for i in range(len(parts)): \n",
    "        sub_part = parts[i] \n",
    "        if num_tokens_from_string(sub_part+chunk) < chunk_size: \n",
    "            chunk+=sub_part \n",
    "            if i == len(parts)-1: \n",
    "                chunk_1st.append(chunk)\n",
    "                break\n",
    "        else:\n",
    "            chunk_1st.append(chunk)\n",
    "            chunk = sub_part\n",
    "            if i == len(parts)-1:\n",
    "                chunk_1st.append(chunk)\n",
    "                break\n",
    "    if num_tokens_from_string(chunk_1st[-1]) < min_chunk_size:\n",
    "        last_chunk = chunk_1st.pop()\n",
    "        chunk_1st[-1] = chunk_1st[-1]+last_chunk\n",
    "    return chunk_1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2851, False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_page_break_pattern(chunk, pattern):\n",
    "    next_page = False\n",
    "    matches = re.finditer(pattern, chunk)\n",
    "    for match in matches:\n",
    "        value = int(match.group(1))\n",
    "        if match.start() == 0:\n",
    "            next_page = True\n",
    "            return value, next_page\n",
    "\n",
    "        return value, next_page\n",
    "\n",
    "    return -1, False\n",
    "\n",
    "pattern = r\"!@#(\\d+)!@#\"\n",
    "\n",
    "find_page_break_pattern('1@#251!@# total_txt !@#2851!@#ewtrqwr',pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_page_num(list_of_chunk_docs):\n",
    "    prev_page=1\n",
    "    page_details = []\n",
    "    final_chunk_1st = []\n",
    "    page_break_pattern = r\"!@#(\\d+)!@#\" \n",
    "    for i in range(len(list_of_chunk_docs)):\n",
    "        chunk = list_of_chunk_docs[i]#.page_content\n",
    "        chunk_without_page_break = re.sub(page_break_pattern, \"\", chunk)\n",
    "        page_num,next_page = find_page_break_pattern(chunk, page_break_pattern)\n",
    "\n",
    "        if page_num == -1:\n",
    "            # final_chunk = Document(page_content=chunk_without_page_break, metadata {\"file_name\":filename, \"page_details\": prev_page))\n",
    "            page_details.append(prev_page)\n",
    "            final_chunk_1st.append(chunk_without_page_break)\n",
    "        else:\n",
    "            if next_page:\n",
    "                page_details.append(page_num+1)\n",
    "                # final_chunk = Document(page_content=chunk_without_page_break, metadata = {\"file_name\":filename, \"page_details\":page_num+1))\n",
    "\n",
    "                final_chunk_1st.append(chunk_without_page_break)\n",
    "            else:\n",
    "                page_details.append(page_num)\n",
    "                #final_chunk = Document(page_content=chunk_without_page_break, metadata (\"file_name\": filename, \"page_details\":page_num))\n",
    "                final_chunk_1st.append(chunk_without_page_break)\n",
    "\n",
    "            prev_page = page_num+1\n",
    "\n",
    "    return final_chunk_1st,page_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([227, 265, 278, 279, 279], [299, 299, 299, 300, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = sorted([num_tokens_from_string(i) for i in chunks])\n",
    "temp[:5],temp[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(chunks,pages,file_name,ids):\n",
    "    metadata = [] # [{'page_no':i} for i in pages]\n",
    "    documents = [] # [doc for doc in chunks]\n",
    "    corpus_json = []\n",
    "\n",
    "    for doc,page_no,id in zip(chunks,pages,ids):\n",
    "        documents.append(doc)\n",
    "        each_metadata = {'page_no':page_no,\"file_name\":file_name}\n",
    "        metadata.append(each_metadata)\n",
    "        each_dict = {'page_content':doc,\"metadata\":{'page_no':page_no,\"file_name\":file_name,\"id\":id}}\n",
    "        corpus_json.append(each_dict)\n",
    "    return documents, metadata, corpus_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qdrant_dense_emd(documents,metadata,ids,emd_path,collection_name):\n",
    "    if os.path.exists(emd_path): \n",
    "        shutil.rmtree(emd_path)\n",
    "\n",
    "    client = QdrantClient(path = emd_path)\n",
    "    client.set_model(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "    if not client.collection_exists(\"startups\"):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=client.get_fastembed_vector_params()\n",
    "        )\n",
    "    # uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "\n",
    "    # metadata = [{'page_no':i} for i in pages]\n",
    "    # documents = [doc for doc in chunks]\n",
    "\n",
    "    client.add(\n",
    "    collection_name=collection_name,\n",
    "    documents=documents,\n",
    "    metadata=metadata,\n",
    "    ids=ids,\n",
    "    parallel=0,  # Use all available CPU cores to encode data.\n",
    "    # Requires wrapping code into if __name__ == '__main__' block\n",
    "    )\n",
    "    return client\n",
    "    \n",
    "def create_bm25s_db(corpus_json):\n",
    "    corpus_text = [doc['page_content'] for doc in corpus_json]\n",
    "    corpus_tokens = bm25s.tokenize(corpus_text,stopwords='en')\n",
    "    retriever = bm25s.BM25(corpus=corpus_json)\n",
    "    retriever.index(corpus_tokens)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rrf(rank_lists, weights, alpha=60, default_rank=1000, k=5):\n",
    "    all_items = set(item for rank_list in rank_lists for item,_ in rank_list)\n",
    "    item_to_index = {item: idx for idx, item in enumerate(all_items)}\n",
    "    rank_matrix = np.full((len(all_items), len(rank_lists)), default_rank)\n",
    "    for list_idx, rank_list in enumerate(rank_lists):\n",
    "        for item, rank in rank_list:\n",
    "            rank_matrix[item_to_index[item], list_idx] = rank\n",
    "\n",
    "    weighted_rrf_scores = np.sum(weights*(1.0/(alpha + rank_matrix)), axis=1)\n",
    "    sorted_indices = np.argsort(-weighted_rrf_scores) # Negative for descending order\n",
    "    sorted_items = [(list(item_to_index.keys()) [idx], weighted_rrf_scores [idx]) for idx in sorted_indices]\n",
    "\n",
    "    return sorted_items[:k]\n",
    "\n",
    "\n",
    "def get_doc_and_source(rrf_retriever, retrieve_doc_dict_keyword, retrieve_doc_dict_sim_search):\n",
    "    final_retrieve_lst = []\n",
    "    unique_source = []\n",
    "    all_source= []\n",
    "\n",
    "    for final_retrieve_doc_with_score in rrf_retriever:\n",
    "        final_retrieve_doc = final_retrieve_doc_with_score[0]\n",
    "        final_retrieve_lst.append(final_retrieve_doc)\n",
    "\n",
    "        if final_retrieve_doc in list(retrieve_doc_dict_keyword.keys()): \n",
    "            source = retrieve_doc_dict_keyword[final_retrieve_doc]\n",
    "            all_source.append(source)\n",
    "            if source not in unique_source:\n",
    "                unique_source.append(source)\n",
    "\n",
    "        elif final_retrieve_doc in list(retrieve_doc_dict_sim_search.keys()):\n",
    "            source = retrieve_doc_dict_sim_search[final_retrieve_doc] \n",
    "            all_source.append(source) \n",
    "            if source not in unique_source:\n",
    "                unique_source.append(source) \n",
    "    return final_retrieve_lst, unique_source, all_source\n",
    "\n",
    "\n",
    "def prepare_retrieve_doc(final_retrieve_lst,all_source):\n",
    "    i = 1\n",
    "    context = \"\"\n",
    "    for doc,source in zip(final_retrieve_lst,all_source):\n",
    "        context += doc.strip(\"\\n\")\n",
    "        context += \"\\n-----------------\\n\"\n",
    "        i+=1\n",
    "    return context.strip(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    }
   ],
   "source": [
    "keyword_retriever = create_bm25s_db(corpus_json)\n",
    "retrieve_doc,retrieve_doc_dict = keyword_search(query,keyword_retriever)\n",
    "\n",
    "# retrieve_doc,retrieve_doc_dict = similarity_search(query,client,collection_name,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query,client,collection_name,k=5):\n",
    "    retrieve_doc = []\n",
    "    retrieve_doc_dict = {}\n",
    "    retrieve_docs = client.query(collection_name = collection_name,query_text = query,limit = k)\n",
    "    rank = 1\n",
    "    for doc in retrieve_docs:\n",
    "        id = doc.id\n",
    "        page_content = doc.metadata['document']\n",
    "        metadata = {'id':id,'page_no':doc.metadata['page_no'],'file_name':doc.metadata['file_name']}\n",
    "        retrieve_doc.append((page_content,rank))\n",
    "        file_name = metadata['file_name']\n",
    "        page_no = metadata['page_no']\n",
    "        path = os.path.join(\"uploaded_files\",file_name)\n",
    "        source = f\"{path}#page={page_no}\"\n",
    "        retrieve_doc_dict[page_content] = source\n",
    "        rank+=1\n",
    "    return retrieve_doc,retrieve_doc_dict\n",
    "\n",
    "def keyword_search(query,keyword_retriever,k=5):\n",
    "    # keyword_retriever = bm25s.BM25.load(path,load_corpus=True)\n",
    "    query_tokens = bm25s.tokenize(query)\n",
    "    results,scores = keyword_retriever.retrieve(query_tokens,k=k)\n",
    "    retrieve_doc = []\n",
    "    retrieve_doc_dict = {}\n",
    "    rank = 1\n",
    "    for doc in results[0]:\n",
    "        page_content = doc['page_content']\n",
    "        metadata = doc['metadata']\n",
    "        retrieve_doc.append((page_content,rank))\n",
    "        file_name = metadata['file_name']\n",
    "        page_no = metadata['page_no']\n",
    "        path = os.path.join(\"uploaded_files\",file_name)\n",
    "        source = f\"{path}#page={page_no}\"\n",
    "        retrieve_doc_dict[page_content] = source\n",
    "        rank+=1\n",
    "    return retrieve_doc,retrieve_doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ensemble_retriever (query,k, weights,client,collection_name,keyword_retriever):\n",
    "    retrieve_doc_sim_search, retrieve_doc_dict_sim_search = similarity_search(query,client,collection_name,k=5)\n",
    "    retrieve_doc_keyword, retrieve_doc_dict_keyword = keyword_search(query,keyword_retriever)\n",
    "\n",
    "    weights = np.array(weights)\n",
    "    rrf_retriever = weighted_rrf([retrieve_doc_keyword, retrieve_doc_sim_search], weights, k=k)\n",
    "    final_retrieve_lst, unique_source, all_source = get_doc_and_source(rrf_retriever, retrieve_doc_dict_keyword, retrieve_doc_dict_sim_search) \n",
    "    retrieve_context = prepare_retrieve_doc(final_retrieve_lst, all_source)\n",
    "\n",
    "    return retrieve_context, unique_source,all_source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"Large language models have been showed to reproduce and amplify biases that are existing in\n",
    "the training data (Sheng et al., 2019; Kurita et al.,\n",
    "2019), and to generate toxic or offensive content (Gehman et al., 2020). \n",
    "\"\"\"\n",
    "retrieve_context, unique_source,all_source = custom_ensemble_retriever(query=query,k=5,weights=weights,\n",
    "                                                            client=client,collection_name=collection_name,\n",
    "                                                            keyword_retriever=keyword_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uploaded_files/temp.pdf#page=7',\n",
       " 'uploaded_files/temp.pdf#page=8',\n",
       " 'uploaded_files/temp.pdf#page=7',\n",
       " 'uploaded_files/temp.pdf#page=11',\n",
       " 'uploaded_files/temp.pdf#page=6']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8                                                    the training data (Sheng et al. 2019; Kurita et al.      PaLM          62B   55.1                                                 2019), and to generate toxic or offensive con-\n",
      "       PaLM-cont       62B   62.8                                                          tent (Gehman et al. 2020). As our training dataset\n",
      "         Chinchilla       70B   67.5                                                   contains a large proportion of data from the Web,      LLaMA         65B   63.4                                    we believe that it is crucial to determine the po-\n",
      "       OPT-IML-Max    30B   43.2              tential for our models to generate such content.       Flan-T5-XXL     11B   55.1          To understand the potential harm of LLaMA-65B,       Flan-PaLM       62B   59.6         we evaluate on different benchmarks that measure\n",
      "        Flan-PaLM-cont   62B   66.1             toxic content production and stereotypes detection.      LLaMA-I        65B   68.9           While we have selected some of the standard bench-\n",
      "                                           marks that are used by the language model com-\n",
      "Table 10: Instruction ﬁnetuning – MMLU (5-shot).   munity to indicate some of the issues with these\n",
      "-----------------\n",
      "    44                                   60                                                         LLaMA 13B                                                                                        70.0                     LLaMA 33B\n",
      "    42                                   55                                      67.5                     LLaMA 65B                                                                                                                                     Chinchilla\n",
      "    40                                   50                                      65.0\n",
      "       0   250  500  750  1000 1250 1500    0   250  500  750  1000 1250 1500    0   250  500  750  1000 1250 1500\n",
      "                  Billion of tokens                           Billion of tokens                           Billion of tokens\n",
      " Figure 2: Evolution of performance on question answering and common sense reasoning during training.5.1  RealToxicityPrompts                                             Basic   Respectful\n",
      "Language models can generate toxic language, e.g.                7B  0.106     0.081\n",
      "insults, hate speech or threats. There is a very large                13B  0.104     0.095\n",
      "                                     LLaMA\n",
      "range of toxic content that a model can generate,               33B  0.107     0.087\n",
      "making a thorough evaluation challenging. Several                65B  0.128     0.141\n",
      "recent work (Zhang et al. 2022; Hoffmann et al.\n",
      "-----------------\n",
      "9% on MMLU.In this section, we show that brieﬂy ﬁnetuning on                                      LLaMA-I (65B) outperforms on MMLU existing\n",
      "instructions data rapidly leads to improvements                                                         instruction ﬁnetuned models of moderate sizes, but\n",
      "on MMLU. Although the non-ﬁnetuned version                                                    are still far from the state-of-the-art, that is 77.4\n",
      "of LLaMA-65B is already able to follow basic in-                                                         for GPT code-davinci-002 on MMLU (numbers\n",
      "structions, we observe that a very small amount of                                                 taken from Iyer et al. (2022)). The details of the\n",
      "ﬁnetuning improves the performance on MMLU,                                              performance on MMLU on the 57 tasks can be\n",
      "and further improves the ability of the model to                                              found in Table 16 of the appendix.follow instructions. Since this is not the focus of\n",
      "this paper, we only conducted a single experiment\n",
      "following the same protocol as Chung et al. (2022)   5  Bias, Toxicity and Misinformation\n",
      "to train an instruct model, LLaMA-I.                                              Large language models have been showed to re-\n",
      "      OPT           30B   26.1                                              produce and amplify biases that are existing in\n",
      "     GLM          120B   44.\n",
      "-----------------\n",
      " 2018; Dai et al. 2019).          sizes and the performance of the system. Kaplan\n",
      "                                                           et al. (2020) derived power laws speciﬁcally for\n",
      "Scaling.  There is a long history of scaling for    transformer based language models, which were\n",
      "language models, for both the model and dataset    later reﬁned by Hoffmann et al. (2022), by adapting\n",
      "sizes. Brants et al. (2007) showed the beneﬁts of    the learning rate schedule when scaling datasets.using language models trained on 2 trillion tokens,    Finally, Wei et al. (2022) studied the effect of scal-\n",
      "resulting in 300 billion n-grams, on the quality of    ing on the abilities of large language models.machine translation. While this work relied on a\n",
      "                                        8  Conclusionsimple smoothing technique, called Stupid Backoff,Heaﬁeld et al. (2013) later showed how to scale    In this paper, we presented a series of language\n",
      "Kneser-Ney smoothing to Web-scale data. This   models that are released openly, and competitive\n",
      "allowed to train a 5-gram model on 975 billions to-   with state-of-the-art foundation models.  Most\n",
      "kens from CommonCrawl, resulting in a model    notably,\n",
      "-----------------\n",
      "                                             HumanEval generations are done in zero-shot and\n",
      "setup as Minerva, with k = 256 samples for MATH\n",
      "                                  MBBP with 3-shot prompts similar to Austin et al.and k = 100 for GSM8k (Minerva 540B uses k = 64\n",
      "                                                         (2021). The values marked with ∗are read from ﬁgures\n",
      "for MATH and and k = 40 for GSM8k). LLaMA-65B\n",
      "                                                             in Chowdhery et al. (2022).outperforms Minerva 62B on GSM8k, although it has\n",
      "not been ﬁne-tuned on mathematical data.                                                    3.6  Massive Multitask Language\n",
      "                                                 Understanding\n",
      "docstring. The model needs to generate a Python\n",
      "program that ﬁts the description and satisﬁes the   The massive multitask language understanding\n",
      "test cases.  In Table 8, we compare the pass@1   benchmark, or MMLU, introduced by Hendrycks\n",
      "scores of our models with existing language mod-    et al. (2020) consists of multiple choice questions\n",
      "els that have not been ﬁnetuned on code, namely   covering various domains of knowledge, includ-\n",
      "PaLM and LaMDA (Thoppilan et al. 2022). PaLM    ing humanities, STEM and social sciences. We\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f8e161c0-b67b-4070-9fa2-b760bab6a8ba',\n",
       "  {'document': 'output. We use the RMSNorm normalizing func-\\ntion, introduced by Zhang and Sennrich (2019).      2.4  Efﬁcient implementation\\nSwiGLU activation function [PaLM]. We re-  We make several optimizations to improve the train-\\nplace the ReLU non-linearity by the SwiGLU ac-   ing speed of our models. First, we use an efﬁcient\\ntivation function, introduced by Shazeer (2020) to   implementation of the causal multi-head attention\\nimprove the performance. We use a dimension of    to reduce memory usage and runtime. This imple-\\n234d instead of 4d as in PaLM.                      mentation, available in the xformers library,2 is\\n                                                     inspired by Rabe and Staats (2021) and uses the\\nRotary Embeddings [GPTNeo]. We remove the                                             backward from Dao et al. (2022). This is achieved\\nabsolute positional embeddings, and instead, add                                          by not storing the attention weights and not com-\\nrotary positional embeddings (RoPE), introduced                                                   puting the key/query scores that are masked due to\\nby Su et al. (2021), at each layer of the network.                                                     the causal nature of the language modeling task.  The details of the hyper-parameters for our dif-                                           To further improve training efﬁciency, we re-\\n',\n",
       "   'page_no': 3,\n",
       "   'file_name': 'temp.pdf'}),\n",
       " ('fccfe080-c6be-441b-99c5-755887ad5243',\n",
       "  {'document': 'ferent models are given in Table 2.                                            duced the amount of activations that are recom-\\n                                                puted during the backward pass with checkpoint-2.3  Optimizer\\n                                                       ing. More precisely, we save the activations that\\nOur models are trained using the AdamW opti-                                                    are expensive to compute, such as the outputs of\\nmizer (Loshchilov and Hutter, 2017), with the fol-                                                        linear layers. This is achieved by manually imple-\\nlowing hyper-parameters: β1 = 0.9, β2 = 0.95.                                               menting the backward function for the transformer\\nWe use a cosine learning rate schedule, such that                                                         layers, instead of relying on the PyTorch autograd.the ﬁnal learning rate is equal to 10% of the maxi-                                          To fully beneﬁt from this optimization, we need to\\nmal learning rate. We use a weight decay of 0.1 and\\ngradient clipping of 1.0. We use 2, 000 warmup        2https://github.com/facebookresearch/xformers\\n                   BoolQ  PIQA  SIQA HellaSwag WinoGrande ARC-e  ARC-c OBQA\\n   GPT-3     175B   60.5    81.0       -      78.9       70.',\n",
       "   'page_no': 3,\n",
       "   'file_name': 'temp.pdf'}),\n",
       " ('639202fb-8586-4578-8133-55d8ae70ff20',\n",
       "  {'document': ' and T5 (Raffel et al.   (2022) that ﬁnetuning these models on instructions\\n2020). A signiﬁcant breakthrough was obtained    lead to promising results, and we plan to further\\nwith GPT-3 (Brown et al. 2020), a model with    investigate this in future work. Finally, we plan to\\n175 billion parameters. This lead to a series of    release larger models trained on larger pretraining\\nLarge Language Models, such as Jurassic-1 (Lieber    corpora in the future, since we have seen a constant\\net al. 2021), Megatron-Turing NLG (Smith et al.   improvement in performance as we were scaling.\\nAcknowledgements                                    Askell,  Sandhini  Agarwal,  Ariel  Herbert-Voss,                                                      Gretchen Krueger, Tom Henighan, Rewon Child,We thank Daniel Haziza, Francisco Massa, Jeremy      Aditya Ramesh, Daniel M. Ziegler,  Jeffrey Wu,Reizenstein, Artem Korenev, and Patrick Labatut      Clemens Winter, Christopher Hesse, Mark Chen,                                                           Eric Sigler, Mateusz Litwin, Scott Gray, Benjaminfrom the xformers team. We thank Susan Zhang\\n                                                        Chess, Jack Clark, Christopher Berner, Sam Mc-\\n',\n",
       "   'page_no': 11,\n",
       "   'file_name': 'temp.pdf'}),\n",
       " ('b4184a2b-a3a7-475b-8914-bbba0d0dac26',\n",
       "  {'document': 'and Stephen Roller for their support on data                                                           Candlish, Alec Radford, Ilya Sutskever, and Dario\\ndeduplication. We thank Luca Wehrstedt, Vegard      Amodei. 2020. Language models are few-shot learn-\\nMella, and Pierre-Emmanuel Mazaré for their        ers.support on training stability. We thank Shubho\\n                                                          Christian Buck, Kenneth Heaﬁeld, and Bas Van Ooyen.Sengupta, Kalyan Saladi, and all the AI infra team                                                       2014. N-gram counts and language models from the\\nfor their support. We thank Jane Yu for her input     common crawl. In LREC, volume 2, page 4.on evaluation. We thank Yongyi Hu for his help\\non data collection.                                    Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,                                                        Thorsten Brants, Phillipp Koehn, and Tony Robin-\\n                                                            son. 2013. One billion word benchmark for measur-\\n                                                          ing progress in statistical language modeling. arXiv\\nReferences                                                             preprint arXiv:1312.3005.Jacob Austin, Augustus Odena, Maxwell Nye, Maarten\\n                                            Mark Chen,  Jerry Tworek, Heewoo  Jun, Qiming  Bosma, Henryk Michalewski, David Dohan,',\n",
       "   'page_no': 12,\n",
       "   'file_name': 'temp.pdf'}),\n",
       " ('35701882-8467-478c-a1aa-02c3f1557875',\n",
       "  {'document': '1     35.1\\n                   13B      45.0       35.8         53.8        53.3     46.9\\n       LLaMA\\n                   33B      55.8       46.0         66.7        63.4     57.8\\n                   65B      61.8       51.7         72.9        67.4     63.4\\n            Table 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.that may indicate that  this benchmark  is not      In Table 10, we report the results of our instruct\\nreliable. On WinoGrande, the performance does   model LLaMA-I on MMLU and compare with ex-\\nnot correlate as well with training perplexity:    isting instruction ﬁnetuned models of moderate\\nthe LLaMA-33B and LLaMA-65B have similar    sizes, namely, OPT-IML (Iyer et al. 2022) and the\\nperformance during the training.                 Flan-PaLM series (Chung et al. 2022). All the re-\\n                                                   ported numbers are from the corresponding papers.4  Instruction Finetuning                     Despite the simplicity of the instruction ﬁnetuning\\n                                              approach used here, we reach 68.',\n",
       "   'page_no': 7,\n",
       "   'file_name': 'temp.pdf'})]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = client.query(collection_name = collection_name,query_text = query,limit = 5)\n",
    "metadata = [(hit.id,hit.metadata) for hit in doc]\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output. We use the RMSNorm normalizing func-\\ntion, introduced by Zhang and Sennrich (2019).      2.4  Efﬁcient implementation\\nSwiGLU activation function [PaLM]. We re-  We make several optimizations to improve the train-\\nplace the ReLU non-linearity by the SwiGLU ac-   ing speed of our models. First, we use an efﬁcient\\ntivation function, introduced by Shazeer (2020) to   implementation of the causal multi-head attention\\nimprove the performance. We use a dimension of    to reduce memory usage and runtime. This imple-\\n234d instead of 4d as in PaLM.                      mentation, available in the xformers library,2 is\\n                                                     inspired by Rabe and Staats (2021) and uses the\\nRotary Embeddings [GPTNeo]. We remove the                                             backward from Dao et al. (2022). This is achieved\\nabsolute positional embeddings, and instead, add                                          by not storing the attention weights and not com-\\nrotary positional embeddings (RoPE), introduced                                                   puting the key/query scores that are masked due to\\nby Su et al. (2021), at each layer of the network.                                                     the causal nature of the language modeling task.  The details of the hyper-parameters for our dif-                                           To further improve training efﬁciency, we re-\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(doc[0].metadata['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(id='5c3cd82d54b947488694965eaff735dc', embedding=None, sparse_embedding=None, metadata={'document': 'output. We use the RMSNorm normalizing func-\\ntion, introduced by Zhang and Sennrich (2019).      2.4  Efﬁcient implementation\\nSwiGLU activation function [PaLM]. We re-  We make several optimizations to improve the train-\\nplace the ReLU non-linearity by the SwiGLU ac-   ing speed of our models. First, we use an efﬁcient\\ntivation function, introduced by Shazeer (2020) to   implementation of the causal multi-head attention\\nimprove the performance. We use a dimension of    to reduce memory usage and runtime. This imple-\\n234d instead of 4d as in PaLM.                      mentation, available in the xformers library,2 is\\n                                                     inspired by Rabe and Staats (2021) and uses the\\nRotary Embeddings [GPTNeo]. We remove the                                             backward from Dao et al. (2022). This is achieved\\nabsolute positional embeddings, and instead, add                                          by not storing the attention weights and not com-\\nrotary positional embeddings (RoPE), introduced                                                   puting the key/query scores that are masked due to\\nby Su et al. (2021), at each layer of the network.                                                     the causal nature of the language modeling task.  The details of the hyper-parameters for our dif-                                           To further improve training efﬁciency, we re-\\n', 'page_no': 3}, document='output. We use the RMSNorm normalizing func-\\ntion, introduced by Zhang and Sennrich (2019).      2.4  Efﬁcient implementation\\nSwiGLU activation function [PaLM]. We re-  We make several optimizations to improve the train-\\nplace the ReLU non-linearity by the SwiGLU ac-   ing speed of our models. First, we use an efﬁcient\\ntivation function, introduced by Shazeer (2020) to   implementation of the causal multi-head attention\\nimprove the performance. We use a dimension of    to reduce memory usage and runtime. This imple-\\n234d instead of 4d as in PaLM.                      mentation, available in the xformers library,2 is\\n                                                     inspired by Rabe and Staats (2021) and uses the\\nRotary Embeddings [GPTNeo]. We remove the                                             backward from Dao et al. (2022). This is achieved\\nabsolute positional embeddings, and instead, add                                          by not storing the attention weights and not com-\\nrotary positional embeddings (RoPE), introduced                                                   puting the key/query scores that are masked due to\\nby Su et al. (2021), at each layer of the network.                                                     the causal nature of the language modeling task.  The details of the hyper-parameters for our dif-                                           To further improve training efﬁciency, we re-\\n', score=0.7772807708572015)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "QdrantClient.search() missing 2 required positional arguments: 'collection_name' and 'query_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m doc\n",
      "\u001b[0;31mTypeError\u001b[0m: QdrantClient.search() missing 2 required positional arguments: 'collection_name' and 'query_vector'"
     ]
    }
   ],
   "source": [
    "doc = client.search(collection_name=\"startups\",query)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearcher:\n",
    "    DENSE_MODEL = \"BAAI/bge-base-en-v1.5\"\n",
    "    # SPARSE_MODEL = \"prithivida/Splade_PP_en_v1\"\n",
    "    def __init__(self, collection_name):\n",
    "        self.collection_name = collection_name\n",
    "        # initialize Qdrant client\n",
    "        self.qdrant_client = QdrantClient(path=\"dense\")\n",
    "        self.qdrant_client.set_model(self.DENSE_MODEL)\n",
    "        # comment this line to use dense vectors only\n",
    "        # self.qdrant_client.set_sparse_model(self.SPARSE_MODEL)\n",
    "    def search(self, text: str):\n",
    "      search_result = self.qdrant_client.query(\n",
    "          collection_name=self.collection_name,\n",
    "          query_text=text,\n",
    "          query_filter=None,  # If you don't want any filters for now\n",
    "          limit=5,  # 5 the closest results\n",
    "      )\n",
    "      # `search_result` contains found vector ids with similarity scores\n",
    "      # along with the stored payload\n",
    "\n",
    "      # Select and return metadata\n",
    "      metadata = [hit.metadata for hit in search_result]\n",
    "      return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Storage folder dense is already accessed by another instance of Qdrant client. If you require concurrent access, use Qdrant server instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBlockingIOError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Debun/agent/venv/lib/python3.10/site-packages/portalocker/portalocker.py:118\u001b[0m, in \u001b[0;36mlock\u001b[0;34m(file_, flags)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mLOCKER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc_value:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Python can use one of several different exception classes to\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# represent timeout (most likely is BlockingIOError and IOError),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# inherit) and check the errno (which should be EACCESS or EAGAIN\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# according to the spec).\u001b[39;00m\n",
      "\u001b[0;31mBlockingIOError\u001b[0m: [Errno 11] Resource temporarily unavailable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAlreadyLocked\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Debun/agent/venv/lib/python3.10/site-packages/qdrant_client/local/qdrant_local.py:134\u001b[0m, in \u001b[0;36mQdrantLocal._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[43mportalocker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flock_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mportalocker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLockFlags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEXCLUSIVE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mportalocker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLockFlags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNON_BLOCKING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m portalocker\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mLockException:\n",
      "File \u001b[0;32m~/Debun/agent/venv/lib/python3.10/site-packages/portalocker/portalocker.py:131\u001b[0m, in \u001b[0;36mlock\u001b[0;34m(file_, flags)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;129;01min\u001b[39;00m (errno\u001b[38;5;241m.\u001b[39mEACCES, errno\u001b[38;5;241m.\u001b[39mEAGAIN):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# A timeout exception, wrap this so the outer code knows to try\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# again (if it wants to).\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mAlreadyLocked(\n\u001b[1;32m    132\u001b[0m         exc_value,\n\u001b[1;32m    133\u001b[0m         fh\u001b[38;5;241m=\u001b[39mfile_,\n\u001b[1;32m    134\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc_value\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Something else went wrong; don't wrap this so we stop\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# immediately.\u001b[39;00m\n",
      "\u001b[0;31mAlreadyLocked\u001b[0m: [Errno 11] Resource temporarily unavailable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWe make several optimizations to improve the training speed of our models. First, we use an efficient implementation of the causal multi-head attention\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m hybrid_searcher \u001b[38;5;241m=\u001b[39m \u001b[43mSemanticSearcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartups\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m hybrid_searcher\u001b[38;5;241m.\u001b[39msearch(text\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m      4\u001b[0m doc\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36mSemanticSearcher.__init__\u001b[0;34m(self, collection_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection_name \u001b[38;5;241m=\u001b[39m collection_name\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# initialize Qdrant client\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqdrant_client \u001b[38;5;241m=\u001b[39m \u001b[43mQdrantClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdense\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqdrant_client\u001b[38;5;241m.\u001b[39mset_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDENSE_MODEL)\n",
      "File \u001b[0;32m~/Debun/agent/venv/lib/python3.10/site-packages/qdrant_client/qdrant_client.py:130\u001b[0m, in \u001b[0;36mQdrantClient.__init__\u001b[0;34m(self, location, url, port, grpc_port, prefer_grpc, https, api_key, prefix, timeout, host, path, force_disable_check_same_thread, grpc_options, auth_token_provider, cloud_inference, check_compatibility, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m QdrantLocal(\n\u001b[1;32m    126\u001b[0m         location\u001b[38;5;241m=\u001b[39mlocation,\n\u001b[1;32m    127\u001b[0m         force_disable_check_same_thread\u001b[38;5;241m=\u001b[39mforce_disable_check_same_thread,\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[43mQdrantLocal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_disable_check_same_thread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_disable_check_same_thread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Debun/agent/venv/lib/python3.10/site-packages/qdrant_client/local/qdrant_local.py:67\u001b[0m, in \u001b[0;36mQdrantLocal.__init__\u001b[0;34m(self, location, force_disable_check_same_thread)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maliases: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flock_file: Optional[TextIOWrapper] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Debun/agent/venv/lib/python3.10/site-packages/qdrant_client/local/qdrant_local.py:139\u001b[0m, in \u001b[0;36mQdrantLocal._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m     portalocker\u001b[38;5;241m.\u001b[39mlock(\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flock_file,\n\u001b[1;32m    136\u001b[0m         portalocker\u001b[38;5;241m.\u001b[39mLockFlags\u001b[38;5;241m.\u001b[39mEXCLUSIVE \u001b[38;5;241m|\u001b[39m portalocker\u001b[38;5;241m.\u001b[39mLockFlags\u001b[38;5;241m.\u001b[39mNON_BLOCKING,\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m portalocker\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mLockException:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStorage folder \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already accessed by another instance of Qdrant client.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If you require concurrent access, use Qdrant server instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Storage folder dense is already accessed by another instance of Qdrant client. If you require concurrent access, use Qdrant server instead."
     ]
    }
   ],
   "source": [
    "query = 'We make several optimizations to improve the training speed of our models. First, we use an efficient implementation of the causal multi-head attention'\n",
    "hybrid_searcher = SemanticSearcher(collection_name=\"startups\")\n",
    "doc = hybrid_searcher.search(text=query)\n",
    "doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
